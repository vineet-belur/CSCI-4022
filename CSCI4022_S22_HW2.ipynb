{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='top'></a>\n",
    "\n",
    "# CSCI4022 Homework 2; Minhashing\n",
    "\n",
    "## Due Friday, February 4 at 11:59 pm to Canvas and Gradescope\n",
    "\n",
    "#### Submit this file as a .ipynb with *all cells compiled and run* to the associated dropbox.\n",
    "\n",
    "***\n",
    "\n",
    "Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your classmates, but **you must write all code and solutions on your own**.\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Any relevant data sets should be available on Canvas. To make life easier on the graders if they need to run your code, do not change the relative path names here. Instead, move the files around on your computer.\n",
    "- If you're not familiar with typesetting math directly into Markdown then by all means, do your work on paper first and then typeset it later.  Here is a [reference guide](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) linked on Canvas on writing math in Markdown. **All** of your written commentary, justifications and mathematical work should be in Markdown.  I also recommend the [wikibook](https://en.wikibooks.org/wiki/LaTeX) for LaTex.\n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do **Kernel $\\rightarrow$ Restart & Run All** as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- It is **bad form** to make your reader interpret numerical output from your code.  If a question asks you to compute some value from the data you should show your code output **AND** write a summary of the results in Markdown directly below your code. \n",
    "- 45 points of this assignment are in problems.  The remaining 5 are for neatness, style, and overall exposition of both code and text.\n",
    "- This probably goes without saying, but... For any question that asks you to calculate something, you **must show all work and justify your answers to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit. \n",
    "- There is *not a prescribed API* for these problems.  You may answer coding questions with whatever syntax or object typing you deem fit.  Your evaluation will primarily live in the clarity of how well you present your final results, so don't skip over any interpretations!  Your code should still be commented and readable to ensure you followed the given course algorithm.\n",
    "- There are two ways to quickly make a .pdf out of this notebook for Gradescope submission.  Either:\n",
    " - Use File -> Download as PDF via LaTeX.  This will require your system path find a working install of a TeX compiler\n",
    " - Easier: Use File ->  Print Preview, and then Right-Click -> Print using your default browser and \"Print to PDF\"\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Problem 3](#p3) |\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unidecode\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p1'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 1 (Theory: minimal Permutations; 9 pts)\n",
    "\n",
    "Consider minhash values for a single column vector that contains 6 components/rows. Four of rows hold 0 and two hold 1. Consider taking all 6! = 720 possible distinct permutations of the six rows. When we choose a permutation of the rows and produce a minhash value for the column, we will use the number of the row, in the permuted order, that is the first with a 1.  Assume that we are 0-indexing when we return the minhash value for a permutation.  Use counting/probability/math in Markdown cells to demonstrate answers to the following, although you may check your work or logic with code/Python.\n",
    "\n",
    "### a) For exactly how many of the 720 permutations is the minhash value for the column a 5?  What proportion/probability is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0/720, it is impossible for the minhash to be 5 as we have 2 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) For exactly how many of the 720 permutations is the minhash value for the column a 4?  What proportion/probability is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "000011 -> 4p4, 2p2, 4!/(4-4)! -> 4!\n",
    "\n",
    "$\\frac{4!2!}{720} = \\frac{48}{720} = \\frac{1}{15}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) For exactly how many of the 720 permutations is the minhash value for the column a 2?  What proportion/probability is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "001(0/1, 0/1, 0/1) -> 4p2, 2p1, 3p3\n",
    "\n",
    "4p2 -> 4!/2!\n",
    "\n",
    "$\\frac{12*2!*2!}{720} = \\frac{144}{720} = \\frac{2}{10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p2'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 2 (Permutations and Hashing; 6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we presented the result that hash functions can behave as **approximate** permutations.  We might, however, be interested in whether or not they are actually randomly selecting from all possible permutations, or if they might somehow be more limited and not \"choose\" some options.\n",
    "\n",
    "### a) Using Markdown cells and as much rigor as you can, prove or disprove the following claim:\n",
    "\n",
    "Given a characteristic matrix with prime $n$ rows, hash functions of the form $h(r)=ar+b \\mod n$ can provide each and every possible permutation of $n$ objects, where $0 < a, b < n$ are integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a proof by contradiction:\n",
    "\n",
    "Lets assume that h(r) = ar+b mod n, where we have n prime rows, and 0 < a,b < n can provide each and every possible permutation of n objects.\n",
    "\n",
    "We know that for any n objects there are n! possible orderings(perm defn -> n!/(n-r)!, n == r(npn) -> n!/1 -> n!).\n",
    "\n",
    "Let's examine the example of n=5 here.\n",
    "\n",
    "There should be a possible 5! orderings here = 120 orderings.\n",
    "\n",
    "Next, we can see that differing values of a/b can correspond to a different ordering of n objects -> simply choose values of a/b, examine buckets -> repeat for different values of a/b.\n",
    "\n",
    "Values of a/b can take on $[1,2,3,4]$. There must be $(n-1)^2$ possible permutations of a and b here.\n",
    "\n",
    "As $4^2 = 16 < 5! = 120$, we can deduce that it is not possible for our hash function h(r) to take on each and every possible permutation of n objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### b) Using Markdown cells, argue in favor or in opposition to the following claim, using as much rigor as you can:\n",
    "Given a characteristic matrix with prime $n$ rows, hash functions of the form $h(r)=ar+b \\mod p$ can provide each and every possible permutation of $n$ objects, where $0 < a, b < p$ are integers and $p>n$ is prime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a value of p that is strictly greater than n and is prime. We are tasked with proving whether or not h(r) = ar+b mod p can provide each and every possible permutation of n objects.\n",
    "\n",
    "In the previous proof we noted that differing values of a/b can take on a possible $(n-1)^2$ orderings. The problem, however, was that the value of n was too small. As we are only told that p has to be strictly greater than n, we can deduce that for any value n we can provide a value of p s.t. $(p-1)^2 >= n!$. \n",
    "\n",
    "One possible refutation to this argument would be that we are interested in generating unique permutations, and if we have large p values, certain values of a/b can generate the exact same permutation(as an ordering with scaled values like 0->2/1->4/2->6 is the same as 0->3/1->5/2->7 in terms of actual execution). As such, we would need a large enough p value and specifically curated values of a/b to generate enough permutations, however as we are not given an upper bound, there should exist some value of p and values of a/b that fulfills n permutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: As always with any proof that's a \"prove or disprove\" claim, try to convince yourself whether the claim is true or false by playing around with small objects, like $n=3$ or $n=5$.  Can you *constructively* make all possible permutations, or are some impossible?\n",
    "\n",
    "Try to be rigorous, but a well thought out written argument will suffice as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p3'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 3 (Applied Minhashing; 30 pts)\n",
    "\n",
    "In this problem we compare similarities of 6 documents available on http://www.gutenberg.org\n",
    "\n",
    "1) The first approximately 10000 characters of Alexander Dumas' *The Count of Monte Cristo*, written in French, in the file `countmc.txt`\n",
    " \n",
    " 2) The first approximately 10000 characters of Victor Hugo *Les Miserables*, written in French, in the file `lesmis.txt`\n",
    " \n",
    " 3) The first approximately 10000 characters of Jules Verne's *20,000 Leagues Under the Sea*, written in French and translated into English by Frederick Paul Walter, in the file `leagues.txt`\n",
    " \n",
    " 4) The first approximately 10000 characters of Kate Chopin's *The Awakening* in the file `awaken.txt`\n",
    " \n",
    " 5) The entirety of around 12000 characters of Kate Chopin's *Beyond the Bayou* in the file `BB.txt`\n",
    " \n",
    " 6) The first approximately 10000 characters of Homer's *The Odyssey*, translated into English by Samuel Butler, in the file `odyssey.txt`\n",
    "\n",
    " \n",
    "### a) Clean the 6 documents, scrubbing all punctuation, changes cases to lower case, and removing accent marks as appropriate.  \n",
    "\n",
    "\n",
    "**For this problem, you may import any text-based packages you desire to help wrangle the data.**  I recommend looking at some functions within `string` or the RegEx `re` packages.\n",
    "\n",
    "You can and probably should use functions in the string package such as `string.lower`, `string.replace`, etc.\n",
    "\n",
    "All 6 documents have been saved in UTF-8 encoding.\n",
    "\n",
    "After processing, you should have (at most) 27 unique characters in each book/section after cleaning, corresponding to white spaces and the 26 letters.  Print out the set of unique characters to ensure this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTF-8 text reading syntax, with new-line replace as white space\n",
    "def clean_book(f):\n",
    "    string = f'../data/{f}.txt'\n",
    "    with open(string, 'r', encoding=\"UTF-8\") as file:\n",
    "        read = file.read().replace('\\n', ' ')\n",
    "        remove_accents = unidecode.unidecode(read)\n",
    "        lower_case = remove_accents.lower()\n",
    "        remove_punctuation = re.sub(r'[^a-z ]', '', lower_case)\n",
    "        remove_gt_one_space = \" \".join(remove_punctuation.split())\n",
    "    return remove_gt_one_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countmc: 25\n",
      "odyssey: 27\n",
      "lesmis: 25\n",
      "leagues: 27\n",
      "BB: 27\n",
      "awaken: 27\n"
     ]
    }
   ],
   "source": [
    "#cursory google search shows letters w/k are rarely used in french - seems fine.\n",
    "file_names = ['countmc', 'odyssey', 'lesmis', 'leagues', 'BB', 'awaken']\n",
    "cleaned_books = []\n",
    "\n",
    "for x in range(0, len(file_names)):\n",
    "    cleaned_books.append(clean_book(file_names[x]))\n",
    "    len_book = set(cleaned_books[x])\n",
    "    print(f\"{file_names[x]}: {len(len_book)}\")\n",
    "cleaned_books = np.array(cleaned_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9832"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_books[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### b) Compute exact similarity scores between the documents.  Are these the expected results?\n",
    "\n",
    "Notes:\n",
    "- You should choose or explore different values of $k$ for your shingles and report the results for multiple values of $k$.  Which values create the largest **range** of similarity scores?\n",
    "- You may choose to shingle on words and create an n-gram model, but it is recommended you shingle on letters as described in class\n",
    "- You may construct your characteristic matrix or characteristic sets with or without hash functions (e.g. by using Python's `set` methods).  Note that choice of hash function should change heavily with $k$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_shingle = 4\n",
    "sim_score_per_book = []\n",
    "\n",
    "def get_sim_score(k_shingle, inp):\n",
    "    start_time = time.time()\n",
    "    shingles_per_inp = [set() for i in range(len(inp))]\n",
    "    sim_score_per_inp = np.full([6, 6], np.nan)\n",
    "    for x in range(0, len(inp)): #for each book\n",
    "        curr_book = inp[x]\n",
    "        for y in range(k_shingle, len(curr_book)): #we can have up to k^27 different k-shingles.\n",
    "            shingles_per_inp[x].add(curr_book[y-k_shingle:y]) #0:k_shingle -> len(curr_book)-k_shingle:len(curr_book)\n",
    "            \n",
    "    '''\n",
    "    (0,1)->(0,5)\n",
    "    (1,2)->(1,5)\n",
    "    (2,3)->(2,5)\n",
    "    (3,4)->(3,5)\n",
    "    (4,5)\n",
    "    '''\n",
    "    for x in range(0, len(inp)-1): #for each input\n",
    "        for y in range(x+1, len(inp)):#for each other input\n",
    "            sim_score_per_inp[x][y] = sim(shingles_per_inp[x], shingles_per_inp[y])\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return shingles_per_inp, sim_score_per_inp\n",
    "\n",
    "def sim(a, b):\n",
    "    union = a.union(b)\n",
    "    intersect = a.intersection(b)\n",
    "    return len(intersect)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0539851188659668 seconds ---\n",
      "--- 0.05466294288635254 seconds ---\n",
      "--- 0.06159472465515137 seconds ---\n",
      "--- 0.0759267807006836 seconds ---\n",
      "--- 0.08907914161682129 seconds ---\n",
      "--- 0.07880187034606934 seconds ---\n",
      "We see the largest range with shingle value: 3, with range: 0.254\n"
     ]
    }
   ],
   "source": [
    "shingles = [2,3,4,5,6,7]\n",
    "ranges = np.empty([len(shingles)])\n",
    "for x in range(0, len(shingles)):\n",
    "    _, sim_scores = get_sim_score(shingles[x], cleaned_books)\n",
    "    ra = np.nanmax(sim_scores)-np.nanmin(sim_scores)\n",
    "    ranges[x] = ra\n",
    "max_range = np.argmax(ranges)\n",
    "print(f\"We see the largest range with shingle value: {shingles[max_range]}, with range: {round(ranges[max_range], 3)}\")\n",
    "#note: our similarity score values are very high with k=3, so we will try k>3 to find more useful sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.09362101554870605 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[       nan, 0.11938681, 0.35514019, 0.14601285, 0.13594369,\n",
       "         0.12874022],\n",
       "        [       nan,        nan, 0.11756549, 0.26715523, 0.30694196,\n",
       "         0.28792726],\n",
       "        [       nan,        nan,        nan, 0.14475121, 0.13232294,\n",
       "         0.12999541],\n",
       "        [       nan,        nan,        nan,        nan, 0.27371061,\n",
       "         0.27408994],\n",
       "        [       nan,        nan,        nan,        nan,        nan,\n",
       "         0.30928331],\n",
       "        [       nan,        nan,        nan,        nan,        nan,\n",
       "                nan]]),\n",
       " array([0.11756549, 0.11938681, 0.12874022, 0.12999541, 0.13232294,\n",
       "        0.13594369, 0.14475121, 0.14601285, 0.26715523, 0.27371061,\n",
       "        0.27408994, 0.28792726, 0.30694196, 0.30928331, 0.35514019,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, s = get_sim_score(4, cleaned_books) \n",
    "#from documentation\n",
    "ind = np.unravel_index(np.argsort(s, axis=None), s.shape)\n",
    "s, s[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Implement minhashing with 1000 hash functions on the 6 documents, checking your results against those in part b).\n",
    "\n",
    "- You may choose your own value of $p$ as the modulus of the hash functions.  You are encouraged to use the example code from the minhashing in class notebook to start you out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash(nhash, dfC):\n",
    "    '''\n",
    "    Takes a number of hash functions to use (nhash) and characteristic matrix (dfC)\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    # use the \"universal hash\":  (a*x+b) mod p, where a, b are random ints and p > N (= 10 here) is prime\n",
    "    np.random.seed(4022) #greatest seed\n",
    "    Ahash = np.random.choice(range(0,10000), size=nhash)\n",
    "    Bhash = np.random.choice(range(0,10000), size=nhash)\n",
    "    Phash = next_prime(len(dfC)) #we can find the next prime number based on our # of objects + func\n",
    "    \n",
    "    # STEP 2:  initialize signature matrix to all infinities\n",
    "\n",
    "    # initialize the signature matrix\n",
    "    Msig = np.full([nhash, len(dfC.columns)], fill_value=np.inf)\n",
    "\n",
    "    # fill in the signature matrix:\n",
    "\n",
    "    # per row of dfC\n",
    "    hash_vals = np.zeros([nhash])\n",
    "    for r in range(len(dfC)):\n",
    "        # get our random a/b values and initialize nhash hashes here. This corresponds to \n",
    "        #generating nhash random permutations.\n",
    "        for h in range(nhash):\n",
    "            hash_vals[h] = (Ahash[h]*r + Bhash[h])%Phash\n",
    "        # STEP 4:  For each column, if there is a 0, do nothing...\n",
    "        for c in range(len(dfC.columns)):\n",
    "            # ... but if there is a 1, replace signature matrix element in that column for each hash fcn \n",
    "            # with the minimum of the hash value in this row, and the current signature matrix element\n",
    "            if dfC.iloc[r,c]==1: #dfc.iloc[r,c]\n",
    "                for h in range(nhash):\n",
    "                    if hash_vals[h] < Msig[h,c]:\n",
    "                        Msig[h,c] = hash_vals[h]\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return Msig\n",
    "\n",
    "def s_score(msig): #just a helper function that calculates the sim score. \n",
    "    #We have summation(i=0 to 5) i possible configs.\n",
    "    s_score = np.full([6, 6], np.nan)\n",
    "    for x in range(0,5): #for each input\n",
    "        for y in range(x+1, 6):#for each other input\n",
    "            s_score[x][y] = sum(msig[:,x]==msig[:,y])/nhash\n",
    "    return s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functionalize for diff k-shingle values.\n",
    "def create_dfC(p):\n",
    "    file_names = ['countmc', 'odyssey', 'lesmis', 'leagues', 'BB', 'awaken']\n",
    "    #make a set containing the union of all sets -> this gives us the number of items that we have.\n",
    " \n",
    "    #now construct a characteristic matrix. \n",
    "    #We know that lookups for sets are O(1), so we should expect decently fast runtime even as we do an 'in' over every set.\n",
    "\n",
    "    start_time = time.time()\n",
    "    #rows == all possible shingles, 6 -> number of docs.\n",
    "    char_mat = np.zeros([len(all_shingles), 6])\n",
    "    #we use enumerate here as sets are unordered. We don't particularily care about order here, just to keep track for the \n",
    "    #char matrix\n",
    "    for i, x in enumerate(all_shingles):\n",
    "        for y in range(0, 6):\n",
    "            if x in p[y]:\n",
    "                #set to 1 if our curr shingle in set of particular document, already is 0 otherwise.\n",
    "                char_mat[i][y] = 1\n",
    "    dfC = pd.DataFrame(char_mat, columns = file_names)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
    "    return dfC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from: https://stackoverflow.com/questions/60003330/find-the-next-prime-number-in-python\n",
    "#felt unimportant to implement myself -> could use an online tool to just gauge this, but wanted to functionalize\n",
    "#so i can run over different shingle values.\n",
    "def is_prime(x):\n",
    "    return all(x % i for i in range(2, x))\n",
    "\n",
    "def next_prime(x):\n",
    "    return min([a for a in range(x+1, 2*x) if is_prime(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.10195565223693848 seconds ---\n",
      "--- 0.07167673110961914 seconds ---\n",
      "--- 134.03538417816162 seconds ---\n"
     ]
    }
   ],
   "source": [
    "nhash = 1000\n",
    "\n",
    "#four_len, four_score = get_sim_score(4, cleaned_books)\n",
    "five_len, five_score = get_sim_score(5, cleaned_books)\n",
    "#six_len, six_score = get_sim_score(6, cleaned_books)\n",
    "\n",
    "#four_df = create_dfC(four_len)\n",
    "five_df = create_dfC(five_len)\n",
    "#six_df = create_dfC(six_len)\n",
    "\n",
    "#msig_four = minhash(nhash, four_df)\n",
    "msig_five = minhash(nhash, five_df)\n",
    "#msig_six = minhash(nhash, six_df)\n",
    "\n",
    "#very slow. I think this is due to using 1000 hashes. \n",
    "#The discrepancy in speed is likely due to initializing all the hashes\n",
    "#if we try to scale the value of k/increase #of docs & size of docs that we have we might see much \n",
    "#slower runtime for the 'Exact Sim Score'\n",
    "#However in this case, the approx sim via minhashing w 1000 hashes is much slower(100x).t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  nan, 0.035, 0.206, 0.044, 0.034, 0.04 ],\n",
       "        [  nan,   nan, 0.024, 0.116, 0.187, 0.155],\n",
       "        [  nan,   nan,   nan, 0.038, 0.032, 0.044],\n",
       "        [  nan,   nan,   nan,   nan, 0.13 , 0.129],\n",
       "        [  nan,   nan,   nan,   nan,   nan, 0.177],\n",
       "        [  nan,   nan,   nan,   nan,   nan,   nan]]),\n",
       " array([[       nan, 0.02862402, 0.21614962, 0.04299743, 0.03666521,\n",
       "         0.03515662],\n",
       "        [       nan,        nan, 0.02741502, 0.13950642, 0.1728981 ,\n",
       "         0.16438356],\n",
       "        [       nan,        nan,        nan, 0.04253889, 0.03565783,\n",
       "         0.03601735],\n",
       "        [       nan,        nan,        nan,        nan, 0.13802358,\n",
       "         0.14550347],\n",
       "        [       nan,        nan,        nan,        nan,        nan,\n",
       "         0.17917485],\n",
       "        [       nan,        nan,        nan,        nan,        nan,\n",
       "                nan]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = s_score(msig_five)\n",
    "\n",
    "fs, five_score\n",
    "#we have relatively comparable values here. for the minhash/exact similarity scores. \n",
    "#highest scores -> (0, 2), (4,5), (1,4), (3,5), (3,4), (1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countmc, odyssey: 0.035, 0.028624022089277498\n",
      "countmc, lesmis: 0.206, 0.21614962481800873\n",
      "countmc, leagues: 0.044, 0.0429974258747259\n",
      "countmc, BB: 0.034, 0.036665211697948494\n",
      "countmc, awaken: 0.04, 0.035156615240766714\n",
      "odyssey, lesmis: 0.024, 0.027415022691488378\n",
      "odyssey, leagues: 0.116, 0.13950642463797674\n",
      "odyssey, BB: 0.187, 0.17289810151104223\n",
      "odyssey, awaken: 0.155, 0.1643835616438356\n",
      "lesmis, leagues: 0.038, 0.04253888995582869\n",
      "lesmis, BB: 0.032, 0.03565782539961356\n",
      "lesmis, awaken: 0.044, 0.03601734867056383\n",
      "leagues, BB: 0.13, 0.13802357902808396\n",
      "leagues, awaken: 0.129, 0.14550346683224671\n",
      "BB, awaken: 0.177, 0.17917485265225933\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 5):\n",
    "    for y in range(x+1, 6):\n",
    "        print(f\"{file_names[x]}, {file_names[y]}: {fs[x][y]}, {five_score[x,y]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The first approximately 10000 characters of Alexander Dumas' The Count of Monte Cristo, written in French, in the file countmc.txt\n",
    "\n",
    "2) The first approximately 10000 characters of Victor Hugo Les Miserables, written in French, in the file lesmis.txt\n",
    "\n",
    "3) The first approximately 10000 characters of Jules Verne's 20,000 Leagues Under the Sea, written in French and translated into English by Frederick Paul Walter, in the file leagues.txt\n",
    "\n",
    "4) The first approximately 10000 characters of Kate Chopin's The Awakening in the file awaken.txt\n",
    "\n",
    "5) The entirety of around 12000 characters of Kate Chopin's Beyond the Bayou in the file BB.txt\n",
    "\n",
    "6) The first approximately 10000 characters of Homer's The Odyssey, translated into English by Samuel Butler, in the file odyssey.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### d) Discussion:\n",
    "\n",
    "Can we detect expected differences here?  Are the two French documents most similar to each other?  Are the two documents by the same author, with the same theme, the most similar?  Is the French-to-English text the most similar English text when compared to the French texts? What kind of alternatives might have captured the structures between these texts?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the highest similarity scores between countmc/lemis, BB/awaken, odyssey/BB, odyssey/awaken. The two french documents and the two documents by the same author are the most similar. Leagues is the most similar to the french texts. Other than this, it seems that the english texts have a lot of commonality, as BB/Odyssey/Leagues/Awaken all have relatively high similarity scores when considered with one another. One important thing might be to remove stop words from the english and french texts and recompute these similarities. We would likely see similar relations, however it would remove even more of the small value similarities. Lastly, it is important to see very low similarity scores between the french and english texts, as it helps validate the large similarity scores we do see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
