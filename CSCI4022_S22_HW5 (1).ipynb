{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='top'></a>\n",
    "\n",
    "# CSCI4022 Homework 5; A-Priori\n",
    "\n",
    "## Due Friday, March 4 at 11:59 pm to Canvas and Gradescope\n",
    "\n",
    "#### Submit this file as a .ipynb with *all cells compiled and run* to the associated dropbox.\n",
    "\n",
    "***\n",
    "\n",
    "Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your classmates, but **you must write all code and solutions on your own**.\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Any relevant data sets should be available on Canvas. To make life easier on the graders if they need to run your code, do not change the relative path names here. Instead, move the files around on your computer.\n",
    "- If you're not familiar with typesetting math directly into Markdown then by all means, do your work on paper first and then typeset it later.  Here is a [reference guide](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) linked on Canvas on writing math in Markdown. **All** of your written commentary, justifications and mathematical work should be in Markdown.  I also recommend the [wikibook](https://en.wikibooks.org/wiki/LaTeX) for LaTex.\n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do **Kernel $\\rightarrow$ Restart & Run All** as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- It is **bad form** to make your reader interpret numerical output from your code.  If a question asks you to compute some value from the data you should show your code output **AND** write a summary of the results in Markdown directly below your code. \n",
    "- 45 points of this assignment are in problems.  The remaining 5 are for neatness, style, and overall exposition of both code and text.\n",
    "- This probably goes without saying, but... For any question that asks you to calculate something, you **must show all work and justify your answers to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit. \n",
    "- There is *not a prescribed API* for these problems.  You may answer coding questions with whatever syntax or object typing you deem fit.  Your evaluation will primarily live in the clarity of how well you present your final results, so don't skip over any interpretations!  Your code should still be commented and readable to ensure you followed the given course algorithm.\n",
    "- There are two ways to quickly make a .pdf out of this notebook for Gradescope submission.  Either:\n",
    " - Use File -> Download as PDF via LaTeX.  This will require your system path find a working install of a TeX compiler\n",
    " - Easier: Use File ->  Print Preview, and then Right-Click -> Print using your default browser and \"Print to PDF\"\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Extra Credit](#p3) |\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import itertools #may use for .combinations/similar, if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p1'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 1 (Practice: Candidate Items; 20 pts)\n",
    "\n",
    "In the A-Priori algorithm, there is a step in which we create a candidate list of frequent itemsets of size $k+1$ as we prune the frequent itemsets of size $k$.  This this problem we will create two functions to do that formally.\n",
    "\n",
    "#### Part A:\n",
    "\n",
    "There are two types of data objects in which we might be holding the frequency counts of itemsets.  If $k=2$, they may be stored in a triangular array.  Create a function `Cand_Trips` that takes a triangular array and returns all valid candidate triples as a list.  Recall that the itemset $\\{i,j,k\\}$ is only a candidate if all 3 of the itemsets in $\\{\\{i,j\\}, \\{i,k\\}, \\{k,j\\}\\}$ are frequent.\n",
    "\n",
    "Some usage notes:\n",
    "\n",
    "- The first input argument is `triang_counts`,  a zero-indexed triangular (numeric) array, by same convention as introduced in class.\n",
    "- The second input argument is the positive integer support threshold `s`.\n",
    "- The underlying itemset is 0-indexed, so e.g. `[0,1,3]` is a valid triple.\n",
    "- You should not convert the input list `triang_counts` into a list of triples as part of your function.\n",
    "- The return array `candidates` should be a list of 3-index lists of the item numbers of the triples.  So a final answer for some input might be:\n",
    "\n",
    "`cand_trips` =\n",
    "    `[[0,3,4], [1,2,7]]`\n",
    "\n",
    "- An implementation note: there are two fundamentally different ways to think about implementing this function.  Option 1 involves thinking about the elements of `triang_counts` in terms of their locations on the corresponding *triangular matrix*: scan row $i$ for a pair of frequent pairs $\\{\\{i,j\\}, \\{i,k\\}\\}$ and then check if $\\{j,k\\}$ is in fact frequent.  Option 2 scans all of `tri_Counts` for frequent item pairs (the \"pruning\" step) and saves those in some object with their indices, then scans *that* object for candidates.  Both are valid for this problem, but option 2 may generalize to higher $k$ better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = lambda i,j,n: int(i*(n-(i+1)/2)+j-i-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index(3,4,5), index(0,1,3)\n",
    "#yay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.union1d([1,0], [0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cand_trips(t_c, s):\n",
    "    num_items = int(len(t_c)/2)\n",
    "    #cant use ndarrays here as we dont know their length.\n",
    "    freq_items = []\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        for j in range(i+1, num_items): #for 5: 1,2,3,4 -> 2,3,4 -> 3,4 -> 4\n",
    "            t_i = index(i,j,num_items)\n",
    "            if t_c[t_i] >= s:\n",
    "                freq_items.append((i,j))\n",
    "                \n",
    "    #iterate over all combinations \n",
    "\n",
    "    for x,y in itertools.combinations(freq_items, 2):\n",
    "        #construct the union of the two tuples here\n",
    "        #we can then check if its len(3) -> if it isnt then we ignore\n",
    "        k_len = list(np.union1d(x,y)) #0(n) here to cast as list, can ignore as len(3)\n",
    "        #means we have 1 common elem\n",
    "        if len(k_len) == 3:\n",
    "            #check if it already is in -> lets not waste time doing expensive set op's otherwise.\n",
    "            if k_len not in candidates:\n",
    "                #ex. (0,1), (0,2) -> need to find (1,2) freq -> is xor1d of two sets(1,0 || 0,1 -> 1)\n",
    "                uniq_vals = tuple(np.setxor1d(x,y)) #0(n) to cast as tuple, we can ignore as len(3)\n",
    "                if uniq_vals in freq_items:\n",
    "                    candidates.append(k_len)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triang_counts=[10,7,3,2,6,4,3,3,6 ,0]\n",
    "cand_trips(triang_counts, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B:\n",
    "\n",
    "A quick test case.  Below is  a matrix $M$ and code including its corresponding the triangular array.  \n",
    "\n",
    "$C=\\begin{bmatrix}\n",
    "\\cdot &10&7&3&2\\\\\n",
    "\\cdot &\\cdot&6&4&3\\\\\n",
    "\\cdot &\\cdot&\\cdot&3&6\\\\\n",
    "\\cdot &\\cdot&\\cdot&\\cdot&0\\\\\n",
    "\\cdot &\\cdot&\\cdot&\\cdot&\\cdot\\\\\n",
    "\\end{bmatrix}$\n",
    " \n",
    "Input the given list into your function to verify that it returns the correct valid triples at $s=1$ and $s=6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For s>=6, candidate: [[0, 1, 2]]\n",
      "For s>=1, candidates: [[0, 1, 2], [0, 1, 3], [0, 1, 4], [0, 2, 3], [0, 2, 4], [1, 2, 3], [1, 2, 4]]\n"
     ]
    }
   ],
   "source": [
    "triang_counts=[10,7,3,2,6,4,3,3,6 ,0]\n",
    "print('For s>=6, candidate:', cand_trips(triang_counts, 6))\n",
    "print('For s>=1, candidates:', cand_trips(triang_counts, 1))\n",
    "\n",
    "#Check that...\n",
    "#cand_trips(triang_counts, 1) returns all the possible triples except those that contain BOTH items 3 and 4.\n",
    "#cand_trips(triang_counts, 6) returns only the triple [[0,1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C:\n",
    "\n",
    "Suppose instead that our $k=2$ item counts were stored in a list of the form e.g.\n",
    "`pairs_counts` =\n",
    "    `[[0,1,12], [0,2,0], [0,3,11], ..., [7,8,103]]`\n",
    "    \n",
    "Where each element is a triple storing the two item indices and their count, $[i,j,c_{ij}]$. \n",
    "\n",
    "Create a function `cand_trips_list` that takes in a list of pairs counts and returns all valid candidate triples as a list.  \n",
    "\n",
    "Some usage notes:\n",
    "\n",
    "- The first input argument is `pairs_counts`,  a zero-indexed list of triples.\n",
    "- The second input argument is the positive integer support threshold `s`.\n",
    "- The underlying itemset is 0-indexed, so e.g. `[0,1,3]` is a valid triple.\n",
    "- The return array `candidates` should be a list of 3-element lists, as above.\n",
    "\n",
    "You should **not** convert the input list `pairs_counts` into a triangular array as part of your function.  After all, sometimes we use the list format for pairs because it saves memory compared to the triangular array format!  You may be able to borrow heavily from the logic of your first function, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#:o\n",
    "def cand_trips_list(pairs_counts, s):\n",
    "    #iterate over all combinations \n",
    "    freq_items = []\n",
    "    candidates = []\n",
    "    \n",
    "    #we just iterate elementwise here.\n",
    "    for x,y,z in pairs_counts:\n",
    "        if z >= s:\n",
    "            freq_items.append((x,y))\n",
    "    \n",
    "    for x,y in itertools.combinations(freq_items, 2):\n",
    "        k_len = list(np.union1d(x,y))\n",
    "        if len(k_len) == 3:\n",
    "            if k_len not in candidates:\n",
    "                uniq_vals = tuple(np.setxor1d(x,y))\n",
    "                if uniq_vals in freq_items:\n",
    "                    candidates.append(k_len)\n",
    "                    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D:\n",
    "\n",
    "Do the test case again.  Below is the list reprentation of the same matrix $M$ from part B.  \n",
    " \n",
    "Input the given list into your function to verify that it returns the correct valid triples at $s=1$ and $s=6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2]]\n",
      "[[0, 1, 2], [0, 1, 3], [0, 1, 4], [0, 2, 3], [0, 2, 4], [1, 2, 3], [1, 2, 4]]\n"
     ]
    }
   ],
   "source": [
    "pairs_counts=[[0,1,10], [0,2,7], [0,3,3], [0,4,2],\\\n",
    "             [1,2,6],[1,3,4], [1,4,3],\\\n",
    "             [2,3,3],[2,4,6],\\\n",
    "             [3,4,0]]\n",
    "print(cand_trips_list(pairs_counts, 6))\n",
    "print(cand_trips_list(pairs_counts, 1))\n",
    "#Check that...\n",
    "#cand_trips(pairs_counts, 1) returns all the possible triples except those that contain BOTH items 3 and 4.\n",
    "#cand_trips(pairs_counts, 6) returns only the triple [[0,1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "\n",
    "Describe *in words* how you would generalize your code in part D to work for generating candidate quadruples $[i_1, i_2, i_3, i_4]$ from an input list of triples counts (each element of the form $[i, j, k, c_{ijk}]$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My list iter here would be different. Would unpack with $x[-1]$ -> count, would take rest($x[:-1]$) and cast as a tuple.\n",
    "\n",
    "I think my lines for generating candidate pairs would work for any k-len tuple. Would have to parameterize my len check(take in k + counter which iterates during a-priori pass in counter.), but rest should be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "<a/ id='p2'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 2 (Practice: A-Priori; 25 pts) \n",
    "\n",
    "Consider the recipe data set provided in `recipes.npy` (use `np.load`).  This includes 100,000 recipes from a variety of sources.\n",
    "\n",
    "We want to use the baskets and the ingredients therein (see `ingredients.npy`) to perform an item basket analysis.\n",
    "\n",
    "This data set is small enough to run directly from main memory, so you may do that if you wish.\n",
    "\n",
    "Loading and accessing the data set is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes=np.load('../recipes.npy', allow_pickle=True)\n",
    "ingredients=np.load('../ingredients.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 233, 2754,   42,  120,  560,  345,  150, 2081,   12,   21])\n",
      " array([ 198,  249,    2,  194, 1884,  791,  965,  423,   53,   48,  798,\n",
      "         31,  362, 1031,   94,   26,    8])]\n",
      "['salt' 'pepper']\n",
      "['basil leaves' 'focaccia' 'leaves' 'mozzarella' 'pesto' 'plum tomatoes'\n",
      " 'rosemary' 'sandwiches' 'sliced' 'tomatoes']\n"
     ]
    }
   ],
   "source": [
    "print(recipes[:2]) #list of lists\n",
    "print(ingredients[:2]) #inventory list\n",
    "print(ingredients[recipes[0]]) #to access a recipe by string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ground'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### a) Since the ingredients file alrady provides integer codes for each of our items, we can move directly into countin via the A-Priori algorithm.  Using the two given files, create a table of frequent single items at 1% support threshold. You may use Python's native classes to set up your lookup functions/tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = lambda i,j,n: int(i*(n-(i+1)/2)+j-i-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_c(t_c,s,m):\n",
    "    freq_items = []\n",
    "    for i in range(m):\n",
    "        for j in range(i+1,m): #for 5: 1,2,3,4 -> 2,3,4 -> 3,4 -> 4\n",
    "            t_i = index(i,j,m)\n",
    "            if t_c[t_i] >= s:\n",
    "                freq_items.append((i,j))\n",
    "    return freq_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s -> supp threshold\n",
    "#k -> number of passes\n",
    "\n",
    "def a_priori(baskets, items, s, k):\n",
    "    s = s*len(baskets) #let s be a fraction of baskets.\n",
    "    counts = np.zeros(len(ingredients))\n",
    "    c = 1\n",
    "    #first pass -----\n",
    "    for x in baskets: \n",
    "        #single recipe\n",
    "        for y in x:\n",
    "            #single ingredient\n",
    "            counts[y] += 1     \n",
    "            \n",
    "    m_items = np.zeros(len(ingredients))\n",
    "    m = 0 #this is the number of frequent singles.\n",
    "    for x in range(len(counts)):\n",
    "        if counts[x] >= s:\n",
    "            m += 1\n",
    "            m_items[x] = m\n",
    "\n",
    "    if k < 2:\n",
    "        #just a helper func to translate to strings\n",
    "        ind = []\n",
    "        for x in range(len(counts)):\n",
    "            if m_items[x] > 0:\n",
    "                ind.append(x)\n",
    "        return ingredients[ind], m_items\n",
    "    \n",
    "    c+=1 #just incrementing to represent num passes\n",
    "    m_c_2 = int(np.ceil((m**2/2)))\n",
    "    count_pairs = np.zeros(m_c_2) \n",
    "\n",
    "    for x in baskets:\n",
    "        #routine to check which values in basket are freq\n",
    "        freq = []\n",
    "        for y in range(0, len(x)):\n",
    "            m_val = m_items[x[y]]\n",
    "            #we need to use the m-value here as we want to make sure\n",
    "            #our indices translate properly to the tri_mat\n",
    "            if m_val > 0:\n",
    "                freq.append(m_val-1) \n",
    "                #we need to minus one as our indices are operating on values being 0 indexed - 1-m is 1 indexed.\n",
    "        #we need to sort frequency here as 0 < i < j <= m. If we dnt sort then i can be g.t. j\n",
    "        #sorting is pretty cheap too.\n",
    "        for i,j in itertools.combinations(sorted(freq), 2):\n",
    "            #calculate pos'n in triangular array. \n",
    "            #increment count\n",
    "            ind = index(i,j,m)\n",
    "            count_pairs[ind] += 1\n",
    "    freq_pairs = tri_c(count_pairs, s, m)\n",
    "    #terribly hard coded but will work\n",
    "    return freq_pairs, m_items, counts, count_pairs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, m, c, c_p = a_priori(recipes, ingredients, 0.1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "c = 0\n",
    "for x in recipes:\n",
    "    if 0 in x and 1 in x:\n",
    "        c += 1\n",
    "print(c_p[0] == c) #index(0,1,m) == 0 -> checking items 0/1 count for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293,\n",
       " array(['salt', 'pepper', 'butter', 'garlic', 'sugar', 'flour', 'onion',\n",
       "        'olive oil', 'water', 'ground', 'olive', 'powder', 'sliced',\n",
       "        'eggs', 'black pepper', 'milk', 'cheese', 'cream', 'lemon',\n",
       "        'chicken', 'sauce', 'tomatoes', 'brown', 'white', 'egg', 'onions',\n",
       "        'vinegar', 'vegetable', 'brown sugar', 'lemon juice',\n",
       "        'ground black pepper', 'parsley', 'cinnamon', 'garlic cloves',\n",
       "        'extract', 'vegetable oil', 'vanilla', 'baking powder',\n",
       "        'vanilla extract', 'unsalted butter', 'ginger', 'chocolate',\n",
       "        'leaves', 'soda', 'parmesan', 'tomato', 'celery', 'potatoes',\n",
       "        'kosher salt', 'mustard', 'cheddar', 'juice', 'baking soda',\n",
       "        'kosher', 'sour cream', 'cilantro', 'soy sauce', 'cream cheese',\n",
       "        'parmesan cheese', 'cheddar cheese', 'oregano', 'red pepper',\n",
       "        'carrots', 'clove', 'chicken broth', 'mushrooms', 'honey',\n",
       "        'packed', 'orange', 'bread', 'thyme', 'oil', 'basil', 'seasoning',\n",
       "        'extra', 'margarine', 'mayonnaise', 'bell pepper', 'cayenne',\n",
       "        'garlic powder', 'nutmeg', 'plus', 'cloves', 'garlic minced',\n",
       "        'white wine', 'bacon', 'peppers', 'ground cinnamon', 'heavy cream',\n",
       "        'boneless', 'garnish', 'paprika', 'black', 'beans', 'toasted',\n",
       "        'virgin olive oil', 'extra virgin olive oil', 'yellow',\n",
       "        'green onions', 'chicken breasts', 'green', 'sesame', 'wine',\n",
       "        'lime juice', 'corn', 'granulated sugar', 'ground pepper',\n",
       "        'coconut', 'spinach', 'ground beef', 'cayenne pepper',\n",
       "        'cornstarch', 'white sugar', 'wine vinegar', 'syrup', 'red onion',\n",
       "        'chili powder', 'rice', 'worcestershire sauce', 'ground cumin',\n",
       "        'mozzarella', 'orange juice', 'grated parmesan cheese', 'red wine',\n",
       "        'walnuts', 'chocolate chips', 'unsweetened', 'pasta',\n",
       "        'pepper flakes', 'minced garlic', 'temperature', 'almonds',\n",
       "        'sea salt', 'yogurt', 'canola', 'carrot', 'chicken stock',\n",
       "        'boneless skinless chicken', 'jalapeno', 'pecans', 'shrimp',\n",
       "        'red pepper flakes', 'wheat', 'dijon mustard', 'pineapple',\n",
       "        'lettuce', 'coriander', 'canola oil', 'dressing', 'chicken breast',\n",
       "        'rosemary', 'mozzarella cheese', 'raisins', 'powdered sugar',\n",
       "        'instant', 'buttermilk', 'sausage', 'red bell pepper',\n",
       "        'dried oregano', 'mushroom', 'sesame oil', 'roasted', 'scallions',\n",
       "        'cumin', 'semisweet chocolate', 'zucchini', 'whipping cream',\n",
       "        'apple', 'confectioners sugar', 'dry', 'crumbs', 'stock',\n",
       "        'peanut butter', 'freshly ground pepper', 'egg yolks', 'pork',\n",
       "        'red', 'cabbage', 'noodles', 'egg whites', 'chili', 'tomato sauce',\n",
       "        'tortillas', 'beef', 'tomato paste', 'chives', 'yeast',\n",
       "        'shortening', 'peas', 'fillets', 'lemon zest', 'olives',\n",
       "        'broccoli', 'cider vinegar', 'cooking spray', 'seeds', 'shallots',\n",
       "        'apples', 'balsamic vinegar', 'breadcrumbs', 'more', 'turkey',\n",
       "        'dry white wine', 'potato', 'green onion', 'prepared',\n",
       "        'light brown sugar', 'strawberries', 'bay leaves', 'dried thyme',\n",
       "        'ground ginger', 'bread crumbs', 'bay leaf', 'cocoa powder',\n",
       "        'lime', 'red wine vinegar', 'skinless chicken breasts', 'ketchup',\n",
       "        'green bell pepper', 'bay', 'black beans', 'pumpkin', 'ice',\n",
       "        'onion powder', 'nuts', 'oats', 'dill', 'allspice',\n",
       "        'boneless skinless chicken breasts', 'sesame seeds',\n",
       "        'green pepper', 'sweetened', 'bell peppers', 'basil leaves', 'ham',\n",
       "        'ground nutmeg', 'maple syrup', 'broth', 'bananas', 'white pepper',\n",
       "        'cucumber', 'jack cheese', 'yellow onion', 'chilies', 'turmeric',\n",
       "        'cake', 'cranberries', 'liquid', 'cold water', 'boiling water',\n",
       "        'monterey jack', 'peanut', 'avocado', 'wheat flour', 'cornmeal',\n",
       "        'hot sauce', 'blueberries', 'dried basil', 'a', 'shallot', 'mint',\n",
       "        'curry powder', 'sherry', 'bouillon', 'coconut milk',\n",
       "        'melted butter', 'mushroom soup', 'green beans', 'spaghetti',\n",
       "        'banana', 'ice cream', 'topping', 'corn syrup', 'parsley leaves',\n",
       "        'feta cheese', 'peppercorns', 'halfandhalf', 'apple cider', 'half',\n",
       "        'semisweet chocolate chips', 'pastry', 'dry yeast', 'ricotta',\n",
       "        'salsa', 'white vinegar', 'cake mix', 'warm water', 'asparagus',\n",
       "        'cherry tomatoes', 'black olives', 'dry mustard', 'green chilies',\n",
       "        'beef broth'], dtype='<U39'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ing, ind = a_priori(recipes, ingredients, 0.01, 1)\n",
    "len(ing), ing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was 1% an appropriate support threshold?  Describe why or why not.  Keep in mind, the goal here is two fold: you want \"actionable\" conclusions, and output that's small enough that you or your grader can make sure that you have the right set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a purely theoretical lense, 1% seems alright for very very large datasets(many components too) that likely do not have much overlap. We expect to have a considerable amount of overlap here. We have 100k recipes and 3.5k ingredients, as such we can expect many ingredients to be featured in at least 1000 buckets. We cannot really draw actionable conclusions with this volume of frequent items.\n",
    "\n",
    "If we wanted to compute frequent pairs we would expect around 293^2/2 -> ~85849 pairs of integers to be loaded into memory as well. This isn't too big of a deal computationally, however if our dataset scaled here it could be a potential issue. One interesting note is that our ingredient list here isn't large enough to provide a memory bottleneck when generating frequent pairs - $2(3500^{2}*32) = 2(3.92*10^{8})$ -> less than a gb. Most modern day computers have around 8gb ram, of which a programmer can expect potentially 1/8th of which to be accessable to a program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### b) Use A-priori to find all frequent  pairs of items from your set of frequent items in a).  Use whatever support threshold you feel is most appropriate, but make sure your result is readable: you should list the top handful of most frequent pairs, sorted by their prevalence.\n",
    "\n",
    "Report the confidences of the two association rules corresponding to the most frequent item pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, m, c, c_p = a_priori(recipes, ingredients, 0.05, 2) #0.05*100000 -> 5000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would have to generate around: 1682 pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"We would have to generate around: {int(max(m)**2/2)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sort = []\n",
    "num_freq = int(max(m))\n",
    "for x in range(0, len(f)):\n",
    "        ind_sort.append((x, c_p[index(f[x][0], f[x][1], num_freq)]))\n",
    "ind_sort.sort(key=lambda x:x[1], reverse=True) #just sort by the second value\n",
    "\n",
    "prevalence = [f[x[0]] for x in ind_sort]\n",
    "\n",
    "#pairs to strings.\n",
    "for x in range(0, len(prevalence)):\n",
    "        ind1 = int(m[m == prevalence[x][0]][0])\n",
    "        ind2 = int(m[m == prevalence[x][1]][0])\n",
    "        ind = ingredients[[ind1,ind2]]\n",
    "        prevalence[x] = ind\n",
    "        \n",
    "#few blocks of code to sort by count + convert indi to strings for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array(['salt', 'pepper'], dtype='<U39'),\n",
       "  array(['olive oil', 'olive'], dtype='<U39'),\n",
       "  array(['pepper', 'garlic'], dtype='<U39'),\n",
       "  array(['salt', 'sugar'], dtype='<U39'),\n",
       "  array(['pepper', 'black pepper'], dtype='<U39'),\n",
       "  array(['pepper', 'ground'], dtype='<U39'),\n",
       "  array(['salt', 'flour'], dtype='<U39'),\n",
       "  array(['salt', 'butter'], dtype='<U39'),\n",
       "  array(['salt', 'garlic'], dtype='<U39'),\n",
       "  array(['butter', 'sugar'], dtype='<U39'),\n",
       "  array(['sugar', 'flour'], dtype='<U39'),\n",
       "  array(['pepper', 'olive'], dtype='<U39'),\n",
       "  array(['pepper', 'olive oil'], dtype='<U39'),\n",
       "  array(['pepper', 'onion'], dtype='<U39'),\n",
       "  array(['butter', 'flour'], dtype='<U39'),\n",
       "  array(['garlic', 'olive'], dtype='<U39'),\n",
       "  array(['garlic', 'olive oil'], dtype='<U39'),\n",
       "  array(['garlic', 'onion'], dtype='<U39'),\n",
       "  array(['salt', 'onion'], dtype='<U39'),\n",
       "  array(['salt', 'olive'], dtype='<U39'),\n",
       "  array(['salt', 'olive oil'], dtype='<U39'),\n",
       "  array(['ground', 'black pepper'], dtype='<U39'),\n",
       "  array(['garlic', 'ground'], dtype='<U39'),\n",
       "  array(['salt', 'water'], dtype='<U39'),\n",
       "  array(['salt', 'ground'], dtype='<U39')],\n",
       " 112)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevalence[:25], len(prevalence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['salt', 'pepper', 'sugar', 'garlic', 'butter'], dtype='<U39')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.flip(ingredients[np.argsort(c)[-5:]]) #gets 5 largest counts, prob a better function to call here but w/e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these associations are quite intuitive, validating the function via an eye test. We might want to calculate confidence and rank by confidence over prevalence, however, as we expect salt/pepper to have the most pairs - they are the most prevalent singletons(added an np.argsort call to display this.). As such, our most prevalent pairs should contain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence(i,j): $conf({I}\\rightarrow{J}) = \\frac{support(I\\cup{J})}{support(I)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salt -> Pepper confidence: [0.53350094]\n",
      "Pepper -> Salt confidence: [0.58468497]\n",
      "Baskets w salt: [0.42163], w pepper: [0.38472]\n"
     ]
    }
   ],
   "source": [
    "#we want the confidence of the two association rules corresponding to the most frequent item pair\n",
    "#therefore we find confidence of i -> salt, j -> pepper; i -> pepper, j -> salt. \n",
    "\n",
    "#first lets strip the item -> index.\n",
    "\n",
    "salt_i = np.where(ingredients == \"salt\")\n",
    "pepper_i = np.where(ingredients == \"pepper\")\n",
    "\n",
    "s_Supp, p_Supp = c[salt_i], c[pepper_i]\n",
    "sp_Supp = c_p[index(m[salt_i[0]]-1, m[pepper_i[0]]-1, num_freq)] #c_p is a triangular matrix, \n",
    "#we technically want the m values here but they are 1 indexed.\n",
    "\n",
    "print(f\"Salt -> Pepper confidence: {sp_Supp/s_Supp}\")\n",
    "print(f\"Pepper -> Salt confidence: {sp_Supp/p_Supp}\")\n",
    "print(f\"Baskets w salt: {s_Supp/len(recipes)}, w pepper: {p_Supp/len(recipes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of confidence here lies in isolating components that are just present in many many many baskets vs those that are in many baskets together. We note that our confidence values are quite high(over 50% for both), which means that there is a clear association between recipes having both salt and pepper. \n",
    "\n",
    "Using some real world extrapolation tells us that most recipes contain both salt and pepper as mandatory seasonings, so this relationship is not all that surprising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**\n",
    "\n",
    "Zach has to go to the store and stock his pantry.  He knows that his girlfriend has a (borderline unhealthy to those around her) love of garlic.  What should he purchase to make sure he has in stock?  What are two most frequent $\\{garlic, x\\}$ item pairs, and what are the two most **interesting** $garlic \\to X$ associations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "garlic_pairs = []\n",
    "for x in prevalence:\n",
    "    if \"garlic\" in x:\n",
    "        garlic_pairs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['pepper', 'garlic'], dtype='<U39'),\n",
       " array(['salt', 'garlic'], dtype='<U39')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garlic_pairs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are both incredibly unsurprising as pepper and salt are the most prevalent singletons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interest(i,j): $int({I}\\rightarrow{J}) = \\frac{support(I\\cup{J})}{support(I)} - P(j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest = lambda ij,i,pj: ij/i - pj #int/int - float -> float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_i = np.zeros(len(garlic_pairs))\n",
    "l_r = len(recipes)\n",
    "for ind,x in enumerate(garlic_pairs):\n",
    "    i,j = np.where(ingredients == x[0])[0][0], np.where(ingredients==x[1])[0][0] \n",
    "    #np.where gives arr(arr(ind, ind, etc.))\n",
    "    i_s, j_s = c[i], c[j]\n",
    "    pj = j_s/l_r\n",
    "    \n",
    "    ij_ind = index(m[i]-1, m[j]-1, num_freq) #1-indexed :L\n",
    "    ij = c_p[ij_ind]\n",
    "    i = interest(ij, i_s, pj)\n",
    "    l_i[ind] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['garlic', 'olive'],\n",
       "        ['garlic', 'olive oil'],\n",
       "        ['pepper', 'garlic'],\n",
       "        ['garlic', 'onion']], dtype='<U39'),\n",
       " array([0.21352845, 0.21294542, 0.21049972, 0.15199928]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp = np.array(garlic_pairs)\n",
    "li_maxi = np.flip(np.argsort(l_i)[-4:])\n",
    "gp[li_maxi], l_i[li_maxi] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Somewhat confused by a few ingredients - might be an issue with how the ingredients were scraped. We see ingredients like ground/olive - these feel like parts of another ingredient.\n",
    "\n",
    "Regardless, we see that garlic and olive/olive oil are the two most interesting relations, with pepper/garlic and garlic/onion right behind. This is pretty straightforward as garlic and olive oil are cornerstones of many american recipes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p3'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 3 (Extra-Credit: A-Priori with hashing and more baskets; 10 pts each part) \n",
    "\n",
    "The data set in 2 had two very appealing propeties that we typically do **not** assume to be the case:\n",
    "- It came with an ingredient list provided\n",
    "- It was small enough to fit into main memory.\n",
    "\n",
    "To fully implement the model, you can get some extra credit by attempting variants of the data that do not have those properties.  We will tackle each problem individually.  You should answer each problem *in its own, separate notebooks* to ensure you're not using any variables from your solution to problem 2 above.\n",
    "\n",
    "## EC1: A-P with hashing\n",
    "\n",
    "#### EC1a) The file `recipesbying` contains the same data set as in problem 2, but the strings themselves live in each recipe.\n",
    "\n",
    "Create a hash table as in nb08 that hashes each ingredient observed based on its string. In other words, create your own version of what **was** in `ingredients.npy` by creating your own hash and/or lookup functions.\n",
    "Include a check to minimize and fix any collisions, as in nb08.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_string(s):\n",
    "    val = 0\n",
    "    for i,x in enumerate(s):\n",
    "        val += (i+1)*ord(x) #note that enumerate is 1 indexed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**EC1b)** Use A-priori to find all frequent items and all frequent pairs of items from your hashed data set in part EC1).  Ensure that the results match those of problem 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2: A-P with massive data\n",
    "\n",
    "The `.npz` file `simplified-recipes-1M.npz` contains over 1 million recipes, and is the original source of the 100,000 recipes used in problem 2.  Using this file (and `ingredients.npy`, if desired), use A-priori to find all frequent items and all frequent pairs of items.  However, you should **not** load all of the file into main memory.  Instead, use `np.memmap` or other options to ensure that you never load into main memory more than 100,000 recipes at a time.  Include any processing in your submission, and use the same proportionate support threshold as you did in problem 2.  Do the most common items differ?\n",
    "\n",
    "A few notes: \n",
    "- If you process the data to make it readable in other forms `.npy`, `.csv`, etc., that's fine, but show all processing code in your submission.\n",
    "- For example, if you find `.memmap` hard to get working, you may convert to `.csv` and use `pd.read_csv` with arguments `chunksize` or `skiprows`, `nrows`\n",
    "- You may be able to do the problem with very little additional work if you are clever about how you open the file and read over it.  In this case, set up your \"loop\" over baskets to only go over 100,000 rows of the file at a time, though, and be very explicit as to how you're avoiding the larger objects ever entering main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
